# perfect_bot_unified.py (FINAL MERGED VERSION: Keywords Filtering + AI Proposals)
import os
import re
import time
import json
import logging
import sqlite3
import threading
from datetime import datetime
from urllib.parse import urljoin
import html

# Third-party imports
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import TimeoutException
from openai import OpenAI

# =================================================================================
# SECTION 1: CONFIGURATION (MERGED FROM OLD CONFIG.PY)
# =================================================================================

# --- SECRETS (âš ï¸ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ) ---
TG_BOT_TOKEN = "8203753766:AAG0wnqXsG_J5dgixGR1GJVnzxfMg6OMXcI"
SAMBANOVA_API_KEY = "bff3f26b-e7f6-4f34-ae84-e849d8d3cab5"

PERMANENT_SUBSCRIBERS = [
    "5030267584",
    "7056782790"
]

# --- BEHAVIOR SETTINGS ---
POLL_INTERVAL_SECONDS = 60
MIN_BUGET_USD = 250
MAX_AGE_HOURS = 24
SEND_SCORE_THRESHOLD = 0.05 # Ø±Ø¬Ø¹Ù†Ø§ Ø§Ù„ÙÙ„ØªØ± (Ø£ÙŠ ÙˆØ¸ÙŠÙØ© Ø³ÙƒÙˆØ±Ù‡Ø§ Ø£Ù‚Ù„ Ù…Ù† ÙƒØ¯Ù‡ Ù…Ø´ Ù‡ØªØªØ¨Ø¹Øª)

# âœ… ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: ØªÙØ¹ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø®ÙÙŠ Ù„Ù„Ø³ÙŠØ±ÙØ±Ø§Øª
HEADLESS_BROWSER = True 

# --- TECHNICAL SETTINGS ---
BASE_URL = "https://mostaql.com"
SEARCH_URL = "https://mostaql.com/projects"
DATABASE_PATH = "mostaql_jobs.db"
CHROME_PROFILE_PATH = "mostaql_chrome_profile"

# --- KEYWORDS FOR JOB SCORING (THE FILTER) ---
# Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø±Ø¬Ø¹ Ø¹Ø´Ø§Ù† ÙŠÙÙ„ØªØ± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
KEYWORDS = {
    "graphics_and_logos": [
        "Graphic Designer", "Logo Designer", "Branding Specialist", "UI/UX Designer", "Social Media Designer",
        "Print Designer", "Illustrator", "Photoshop Expert", "Illustrator Expert", "Figma Designer",
        "Corporate Identity", "Brand Style Guide", "UI Kit", "UX Research", "Wireframing", "Prototyping",
        "Social Media Kit", "Ad Creative Design", "Infographic Design", "Photo Editor",
        "Ù…ØµÙ…Ù… Ø¬Ø±Ø§ÙÙŠÙƒ", "Ù…Ø·Ù„ÙˆØ¨ Ù…ØµÙ…Ù… Ø´Ø¹Ø§Ø±Ø§Øª", "ØªØµÙ…ÙŠÙ… Ù‡ÙˆÙŠØ© Ø¨ØµØ±ÙŠØ© ÙƒØ§Ù…Ù„Ø©", "Ù…ØµÙ…Ù… ÙˆØ§Ø¬Ù‡Ø§Øª Ù…Ø³ØªØ®Ø¯Ù… UI/UX",
        "ØªØµØ§Ù…ÙŠÙ… Ø³ÙˆØ´ÙŠØ§Ù„ Ù…ÙŠØ¯ÙŠØ§", "Ø®Ø¨ÙŠØ± ÙÙˆØªÙˆØ´ÙˆØ¨", "Ù…Ø­ØªØ±Ù Ø§Ù„ÙŠØ³ØªØ±ÙŠØªÙˆØ±", "ØªØµÙ…ÙŠÙ… Ø¨ÙˆØ³ØªØ§Øª Ø§Ù†Ø³ØªØºØ±Ø§Ù…",
        "ØªØµÙ…ÙŠÙ… Ù‡ÙˆÙŠØ© Ø´Ø±ÙƒØ©", "Ø¨Ø±ÙˆÙØ§ÙŠÙ„ Ø´Ø±ÙƒØ©", "ØªØµÙ…ÙŠÙ… Ø¥Ø¹Ù„Ø§Ù†Ø§Øª Ù…Ù…ÙˆÙ„Ø©", "Ù…ØµÙ…Ù… Ø£ÙŠÙ‚ÙˆÙ†Ø§Øª", "ØªØ¹Ø¯ÙŠÙ„ ØµÙˆØ±",
        "Ø±Ø³Ù… ÙÙŠÙƒØªÙˆØ±", "ØªØµÙ…ÙŠÙ… ÙƒØ±ÙˆØª Ø´Ø®ØµÙŠØ©", "ØªØµÙ…ÙŠÙ… Ø¨Ø±ÙˆØ´ÙˆØ± ÙˆÙÙ„Ø§ÙŠØ±", "ØªØµÙ…ÙŠÙ… Ù„Ø§ÙØªØ§Øª"
    ],
    "motion_and_video": [
        "Video Editor", "Motion Graphics Artist", "2D Animator", "Video Producer", "Reels/Shorts Editor",
        "YouTube Video Editor", "Explainer Video Creator", "Logo Animation", "Video Ad Creator",
        "After Effects Expert", "Premiere Pro Expert", "Color Grading", "Sound Design",
        "Ù…ÙˆÙ†ØªÙŠØ±", "Ù…Ø­Ø±Ø± ÙÙŠØ¯ÙŠÙˆ", "Ù…ÙˆØ´Ù† Ø¬Ø±Ø§ÙÙŠÙƒ", "Ù…ØµÙ…Ù… ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª", "ØªØ¹Ø¯ÙŠÙ„ ÙÙŠØ¯ÙŠÙˆ", "ØµØ§Ù†Ø¹ Ù…Ø­ØªÙˆÙ‰ ÙÙŠØ¯ÙŠÙˆ",
        "Ù…ÙˆÙ†ØªØ§Ø¬ ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ÙŠÙˆØªÙŠÙˆØ¨", "ÙÙŠØ¯ÙŠÙˆ Ø¥Ø¹Ù„Ø§Ù†ÙŠ", "ØªØµÙ…ÙŠÙ… ÙÙŠØ¯ÙŠÙˆ Ù…ÙˆØ´Ù† Ø¬Ø±Ø§ÙÙŠÙƒ", "Ø®Ø¨ÙŠØ± After Effects",
        "Ù…Ø­ØªØ±Ù Ø¨Ø±ÙŠÙ…ÙŠØ± Ø¨Ø±Ùˆ", "Ø¹Ù…Ù„ Ø§Ù†ØªØ±Ùˆ Ø§Ø­ØªØ±Ø§ÙÙŠ", "ØªØ­Ø±ÙŠÙƒ Ø´Ø¹Ø§Ø±", "ØªØµØ­ÙŠØ­ Ø£Ù„ÙˆØ§Ù† Ø§Ù„ÙÙŠØ¯ÙŠÙˆ", "Ù‡Ù†Ø¯Ø³Ø© ØµÙˆØªÙŠØ© Ù„Ù„ÙÙŠØ¯ÙŠÙˆ"
    ],
    "3d_design": [
        "3D Artist", "3D Modeler", "3D Generalist", "Architectural Visualizer", "Product Renderer",
        "Character Artist", "3D Animator", "VFX Artist", "Blender Expert", "3ds Max Specialist",
        "CGI Artist", "3D Product Mockup", "Interior/Exterior Rendering",
        "Ù…ØµÙ…Ù… Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯", "Ù†Ù…Ø°Ø¬Ø© 3D", "ØªØµÙ…ÙŠÙ… Ù…Ù†ØªØ¬Ø§Øª 3D", "Ø¥Ø¸Ù‡Ø§Ø± Ù…Ø¹Ù…Ø§Ø±ÙŠ", "Ø±Ù†Ø¯Ø± Ù…Ø¹Ù…Ø§Ø±ÙŠ",
        "ØªØµÙ…ÙŠÙ… Ø´Ø®ØµÙŠØ§Øª 3D", "Ø®Ø¨ÙŠØ± Ø¨Ù„Ù†Ø¯Ø±", "Ù…Ø­ØªØ±Ù 3ds Max", "ØªØµÙ…ÙŠÙ… Ù…Ø¬Ø³Ù…Ø§Øª", "Ù…Ø´Ù‡Ø¯ Ø«Ù„Ø§Ø«ÙŠ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯",
        "ØªØµÙ…ÙŠÙ… Ø¯ÙŠÙƒÙˆØ± Ø¯Ø§Ø®Ù„ÙŠ 3D"
    ],
    "web_development": [
        "Front-End Developer", "Back-End Developer", "Full-Stack Developer", "WordPress Developer",
        "Shopify Expert", "Web Designer", "React Developer", "PHP Laravel Developer", "Node.js Developer",
        "E-commerce Website", "Landing Page Design", "Website Customization", "Bug Fixes", "API Integration",
        "Ù…Ø·ÙˆØ± Ù…ÙˆØ§Ù‚Ø¹", "Ù…Ø¨Ø±Ù…Ø¬ ÙˆÙŠØ¨", "Ù…Ø·ÙˆØ± ÙˆØ§Ø¬Ù‡Ø§Øª Ø£Ù…Ø§Ù…ÙŠØ©", "Ù…Ø·ÙˆØ± Back-End", "Ø®Ø¨ÙŠØ± ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³",
        "Ù…Ø·Ù„ÙˆØ¨ Ù…Ø¨Ø±Ù…Ø¬ PHP Laravel", "Ø¥Ù†Ø´Ø§Ø¡ Ù…ØªØ¬Ø± Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ", "ØªØµÙ…ÙŠÙ… ÙˆØ¨Ø±Ù…Ø¬Ø© Ù…ÙˆÙ‚Ø¹", "ØªØ·ÙˆÙŠØ± Ù…ØªØ¬Ø± Ø´ÙˆØ¨ÙŠÙØ§ÙŠ",
        "Ù…Ø·ÙˆØ± Full-Stack", "ØªØµÙ…ÙŠÙ… ØµÙØ­Ø© Ù‡Ø¨ÙˆØ·", "ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ù‚Ø§Ø¦Ù…", "Ø±Ø¨Ø· Ø¨ÙˆØ§Ø¨Ø§Øª Ø§Ù„Ø¯ÙØ¹", "Ø®Ø¨ÙŠØ± React.js"
    ],
    "mobile_app_development": [
        "Flutter Developer", "Mobile App Developer", "iOS Developer", "Android Developer",
        "Cross-Platform Developer", "React Native Developer", "Mobile UI/UX Designer",
        "App Development", "Build an App", "API Integration for App", "Firebase Expert",
        "Ù…Ø¨Ø±Ù…Ø¬ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙ„Ø§ØªØ±", "Ù…Ø·ÙˆØ± ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¬ÙˆØ§Ù„", "ØªØµÙ…ÙŠÙ… ÙˆØ¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚", "Ù…Ø·Ù„ÙˆØ¨ Ù…Ø¨Ø±Ù…Ø¬ ØªØ·Ø¨ÙŠÙ‚Ø§Øª",
        "Ø®Ø¨ÙŠØ± Flutter", "Ù…Ø¨Ø±Ù…Ø¬ iOS", "Ù…Ø¨Ø±Ù…Ø¬ Android", "ØªØ·Ø¨ÙŠÙ‚ Ù„Ù…ØªØ¬Ø± Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ", "Ø±Ø¨Ø· ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹ Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ…",
        "ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø§Ù„ØµÙØ±"
    ],
    "voice_over": [
        "Voice Over Artist", "Voice Actor", "Narrator", "Dubbing Artist", "Audiobook Narrator",
        "Commercial Voice Over", "IVR System Voice", "Arabic Voice Over", "English Voice Over",
        "Ù…Ø¹Ù„Ù‚ ØµÙˆØªÙŠ", "ÙÙˆÙŠØ³ Ø£ÙˆÙØ±", "ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ Ø§Ø­ØªØ±Ø§ÙÙŠ", "Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„ØµÙˆØªÙŠ", "Ù…Ø·Ù„ÙˆØ¨ ÙÙˆÙŠØ³ Ø£ÙˆÙØ±",
        "ØªØ³Ø¬ÙŠÙ„ Ø¥Ø¹Ù„Ø§Ù† ØµÙˆØªÙŠ", "Ø¯Ø¨Ù„Ø¬Ø© Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠØ¯ÙŠÙˆ", "Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰", "ÙÙˆÙŠØ³ Ø£ÙˆÙØ± Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©",
        "ØªØ³Ø¬ÙŠÙ„ ÙƒØªØ§Ø¨ ØµÙˆØªÙŠ", "Ø§Ù„Ø±Ø¯ Ø§Ù„Ø¢Ù„ÙŠ IVR"
    ],
    "ai_services": [
        "AI Specialist", "Machine Learning Engineer", "AI Content Creator", "Generative AI Artist",
        "Chatbot Developer", "AI Integration", "Stable Diffusion Expert", "Midjourney Artist",
        "Ø®Ø¨ÙŠØ± Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ù…ØªØ®ØµØµ Midjourney", "Ø¨Ù†Ø§Ø¡ Ø´Ø§Øª Ø¨ÙˆØª",
        "ØªØ·ÙˆÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©", "ÙƒØªØ§Ø¨Ø© Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", "Ø¯Ù…Ø¬ Ø®Ø¯Ù…Ø§Øª AI", "Ø£ØªÙ…ØªØ© Ù…Ù‡Ø§Ù… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"
    ],
    "print_and_packaging": [
        "Packaging Designer", "Book Cover Designer", "Layout Designer", "Print-Ready Files",
        "Publication Design", "Label Designer", "Box Packaging", "Die-line creation",
        "ØªØµÙ…ÙŠÙ… Ø£ØºÙ„ÙØ© ÙƒØªØ¨", "ØªØµÙ…ÙŠÙ… Ø¹Ø¨ÙˆØ§Øª Ù…Ù†ØªØ¬Ø§Øª", "ØªØºÙ„ÙŠÙ Ù…Ù†ØªØ¬", "ØªØµÙ…ÙŠÙ… Ù…Ù„ØµÙ‚Ø§Øª", "ØªÙ†Ø³ÙŠÙ‚ ÙƒØªØ§Ø¨ Ù„Ù„Ø·Ø¨Ø§Ø¹Ø©",
        "ØªØµÙ…ÙŠÙ… Ù…Ø¬Ù„Ø§Øª", "ØªØµÙ…ÙŠÙ… Ø¹Ù„Ø¨", "ØªØ¬Ù‡ÙŠØ² Ù…Ù„ÙØ§Øª Ù„Ù„Ø·Ø¨Ø§Ø¹Ø©", "ØªØµÙ…ÙŠÙ… Ù‚Ø§Ø¦Ù…Ø© Ø·Ø¹Ø§Ù… (Ù…Ù†ÙŠÙˆ)"
    ]
}

# =================================================================================
# SECTION 2: AI BRAIN (The Perfect Personas & Classy Intro)
# =================================================================================

client_ai = None
if SAMBANOVA_API_KEY and "xxxx" not in SAMBANOVA_API_KEY:
    try:
        client_ai = OpenAI(
            api_key=SAMBANOVA_API_KEY,
            base_url="https://api.sambanova.ai/v1",
        )
    except Exception as e:
        print(f"âš ï¸ Error initializing AI: {e}")

PROPOSAL_SYSTEM_PROMPT = """
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙˆÙ…Ø³ØªØ´Ø§Ø± Ù…Ø­ØªØ±Ù (Top Rated Plus) ÙÙŠ Ù…Ø¬Ø§Ù„ÙƒØŒ ÙˆÙ„Ø¯ÙŠÙƒ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ Ø¨Ù‚Ø¯Ø±Ø§ØªÙƒ.
Ù…Ù‡Ù…ØªÙƒ ÙƒØªØ§Ø¨Ø© Ø¹Ø±Ø¶ ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† "Ø§Ù„Ù‚ÙˆØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©" Ùˆ"Ø§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø©" Ù„Ù„Ø¹Ù…ÙŠÙ„.

ğŸ’ **Ø§Ù„Ø´Ø®ØµÙŠØ§Øª (Ø®Ø¨Ø±Ø§Ø¡ Ø¨Ø®Ø¨Ø±Ø© +10 Ø³Ù†ÙˆØ§Øª):**
1. **[Ù…Ø¹Ù…Ø§Ø±ÙŠ/Ø¯ÙŠÙƒÙˆØ±]** -> **Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³Ø© Ø£Ù…Ù„ Ù…Ø­Ù…Ø¯**. (Ø§Ù„Ù„ØºØ©: Ù‡Ù†Ø¯Ø³ÙŠØ©ØŒ Ø±Ø§Ù‚ÙŠØ©).
2. **[ÙÙŠØ¯ÙŠÙˆ/Ù…ÙˆØ´Ù†]** -> **Ø§Ù„Ù…ØµÙ…Ù… Ø£Ø­Ù…Ø¯ Ù…Ø­Ù…Ø¯**. (Ø§Ù„Ù„ØºØ©: Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©ØŒ Ø¬Ø°Ø§Ø¨Ø©).
3. **[Ø¬Ø±Ø§ÙÙŠÙƒ/Ù‡ÙˆÙŠØ©]** -> **Ø§Ù„Ù…ØµÙ…Ù…Ø© Ø£Ù…ÙŠØ±Ø© Ù…Ø­Ù…Ø¯**. (Ø§Ù„Ù„ØºØ©: ÙÙ†ÙŠØ©ØŒ ØªØ³ÙˆÙŠÙ‚ÙŠØ©).
4. **[Ø¨Ø±Ù…Ø¬Ø©/ØªÙ‚Ù†ÙŠØ©]** -> **Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ Ù…Ø­Ù…Ø¯ Ø³Ù„ÙŠÙ…Ø§Ù†**. (Ø§Ù„Ù„ØºØ©: Ø¯Ù‚ÙŠÙ‚Ø©ØŒ ØªÙ‚Ù†ÙŠØ©).

ğŸ”¥ **Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ Ù„Ù„Ø±Ø³Ø§Ù„Ø© (Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„ØµÙŠØ§ØºØ©):**

1. **Ø§Ù„ØªØ±Ø­ÙŠØ¨:** (Ù…Ø±Ø­Ø¨Ø§ Ø£Ø³ØªØ§Ø° [Ø§Ù„Ø§Ø³Ù…]).

2. **Ø§Ù„ØªØ¹Ø±ÙŠÙ + Ø§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø© (Ø£Ù‡Ù… Ø¬Ø²Ø¡):**
   - Ø§Ø¨Ø¯Ø£ Ø¨Ù€: "Ù…Ø¹Ùƒ [Ø§Ù„Ø§Ø³Ù…]ØŒ [Ø§Ù„ØµÙØ©] Ø¨Ø®Ø¨Ø±Ø© ØªØªØ¬Ø§ÙˆØ² 10 Ø³Ù†ÙˆØ§Øª ÙÙŠ [Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø¯Ù‚ÙŠÙ‚]."
   - **Ø«Ù… ÙÙˆØ±Ø§Ù‹ Ø§ÙƒØªØ¨ Ø¬Ù…Ù„Ø© ØªØ£ÙƒÙŠØ¯ Ø§Ù„ÙÙ‡Ù…:** "Ø£Ø¤ÙƒØ¯ Ù„Ùƒ Ø£Ù†Ù†ÙŠ ÙÙ‡Ù…Øª ØªÙ…Ø§Ù…Ø§Ù‹ Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…Ø´Ø±ÙˆØ¹Ùƒ ÙˆÙ…ØªØ·Ù„Ø¹Ø§ØªÙƒ Ø§Ù„Ø¹Ø§Ù„ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ [Ù‡Ø¯Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹] Ø¨Ø¬ÙˆØ¯Ø© ÙØ§Ø¦Ù‚Ø© ÙˆÙˆØ§Ù‚Ø¹ÙŠØ©."

3. **Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ (Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø¹Ø¶Ù„Ø§Øª):**
   - "Ù„Ù‚Ø¯ Ù„ÙØª Ø§Ù†ØªØ¨Ø§Ù‡ÙŠ ÙÙŠ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø£Ù†Ùƒ ØªØ­ØªØ§Ø¬..." (ØªØ­Ø¯Ø« Ù‡Ù†Ø§ Ø¹Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©).

4. **Ø§Ù„Ø­Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­:** (Ø§Ø´Ø±Ø­ ÙƒÙŠÙ Ø³ØªÙ†ÙØ° Ø§Ù„Ø¹Ù…Ù„).

5. **Ø§Ù„Ø®Ø§ØªÙ…Ø© (Classy Closing - Ø´ÙŠÙƒ Ø¬Ø¯Ø§Ù‹):**
   - Ø§Ø³ØªØ®Ø¯Ù… ØµÙŠØº Ù…Ø«Ù„: "ÙŠØ³Ø¹Ø¯Ù†ÙŠ Ø¬Ø¯Ø§Ù‹ Ø£Ù† Ø£ÙƒÙˆÙ† Ø´Ø±ÙŠÙƒØ§Ù‹ ÙÙŠ Ù†Ø¬Ø§Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"ØŒ "Ø¨Ø§Ù†ØªØ¸Ø§Ø± ØªÙˆØ§ØµÙ„ÙƒÙ… Ø§Ù„ÙƒØ±ÙŠÙ… Ù„Ù„Ø¨Ø¯Ø¡ Ø¹Ù„Ù‰ Ø¨Ø±ÙƒØ© Ø§Ù„Ù„Ù‡".

âš ï¸ **ØªÙ†Ø¨ÙŠÙ‡Ø§Øª:**
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ù…Ù‚Ø¯Ù…Ø§Øª Ø¥Ù†Ø´Ø§Ø¦ÙŠØ© ÙØ§Ø±ØºØ©.
- Ø§Ù„ÙƒÙ„Ø§Ù… Ù…ÙˆØ¬Ù‡ Ø¨ØµÙŠØºØ© Ø§Ù„Ù…ÙØ±Ø¯ Ø§Ù„Ù…Ø°ÙƒØ±.
"""

def generate_ai_proposal(job_description, client_name=""):
    """Ø¯Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Llama 3.3"""
    if not client_ai: return "âš ï¸ AI Key missing."
    
    clean_name = client_name.split()[0] if client_name else "Ø¹Ø²ÙŠØ²ÙŠ"
    clean_name = re.sub(r'[^\w\s]', '', clean_name).strip()

    try:
        user_content = (
            f"Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ„: {clean_name}\n"
            f"ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:\n{job_description}"
        )
        response = client_ai.chat.completions.create(
            model="Meta-Llama-3.3-70B-Instruct",
            messages=[
                {"role": "system", "content": PROPOSAL_SYSTEM_PROMPT},
                {"role": "user", "content": user_content}
            ],
            temperature=0.3,
            top_p=0.9
        )
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f"AI Generation Error: {e}")
        return "âŒ Could not generate proposal."

# =================================================================================
# SECTION 3: INITIAL SETUP & DATABASE
# =================================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] (%(threadName)s) %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("bot.log", "a", "utf-8")]
)

DB_CONNECTION_LOCK = threading.Lock()

def get_db_connection():
    conn = sqlite3.connect(DATABASE_PATH, timeout=10, check_same_thread=False)
    return conn

def init_db(conn):
    logging.info("Initializing database...")
    with DB_CONNECTION_LOCK:
        cur = conn.cursor()
        cur.execute("""
        CREATE TABLE IF NOT EXISTS jobs (
            id TEXT PRIMARY KEY, title TEXT NOT NULL, url TEXT NOT NULL,
            first_seen TIMESTAMP NOT NULL, was_sent INTEGER DEFAULT 0,
            was_applied INTEGER DEFAULT 0, score REAL DEFAULT 0, max_budget REAL
        )""")
        cur.execute("CREATE TABLE IF NOT EXISTS subscribers (chat_id TEXT PRIMARY KEY)")
        conn.commit()
    logging.info("Database initialized successfully.")

def get_unseen_job_ids(conn, job_ids):
    if not job_ids: return []
    with DB_CONNECTION_LOCK:
        placeholders = ','.join('?' for _ in job_ids)
        query = f"SELECT id FROM jobs WHERE id IN ({placeholders})"
        cur = conn.cursor()
        cur.execute(query, job_ids)
        seen_ids = {row[0] for row in cur.fetchall()}
    return [job_id for job_id in job_ids if job_id not in seen_ids]

# =================================================================================
# SECTION 4: SCORING ENGINE (RESTORED FROM OLD CODE)
# =================================================================================
def normalize_text(text):
    if not text: return ""
    text = str(text).lower()
    # Simple normalization for matching
    text = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù€]", "", text) 
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§").replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    return text

def calculate_relevance_score(text):
    """
    ÙŠÙ‚ÙˆÙ… Ø¨Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙŠ KEYWORDS
    """
    normalized_text = normalize_text(text)
    best_score = 0.0
    best_category = "General"
    matched_keywords = []

    for category, keywords in KEYWORDS.items():
        for keyword in keywords:
            normalized_keyword = normalize_text(keyword)
            # Check for exact word match or substring
            if normalized_keyword in normalized_text:
                # Ù„Ùˆ Ù„Ù‚Ù‰ ÙƒÙ„Ù…Ø©ØŒ ÙŠØ¯ÙŠÙ„Ù‡ Ø³ÙƒÙˆØ± Ø¹Ø§Ù„ÙŠ (Ù…Ø«Ù„Ø§Ù‹ 0.8)
                # Ù…Ù…ÙƒÙ† Ù†Ø²ÙˆØ¯ Ø§Ù„Ø³ÙƒÙˆØ± Ù„Ùˆ Ù„Ù‚Ù‰ Ø£ÙƒØªØ± Ù…Ù† ÙƒÙ„Ù…Ø©ØŒ Ø¨Ø³ Ù„Ù„ØªØ¨Ø³ÙŠØ· Ù‡Ù†Ø¹ØªØ¨Ø± Ø£ÙŠ ÙƒÙ„Ù…Ø© ÙƒÙØ§ÙŠØ©
                score = 0.8 
                if score > best_score:
                    best_score = score
                    best_category = category
                matched_keywords.append(keyword)
    
    # Ù„Ùˆ Ù…Ù„Ù‚Ø§Ø´ Ø£ÙŠ ÙƒÙ„Ù…Ø©ØŒ Ø§Ù„Ø³ÙƒÙˆØ± Ù‡ÙŠØ¨Ù‚Ù‰ 0.0
    return best_score, matched_keywords, best_category

# =================================================================================
# SECTION 5: WEB SCRAPING
# =================================================================================
def perform_login(driver):
    logging.info("Checking login status...")
    try:
        driver.get(urljoin(BASE_URL, "/projects")); time.sleep(3)
        if "/login" in driver.current_url:
            logging.info("Not logged in. Proceeding with login form...")
            # âš ï¸ Note: Make sure MOSTAQL_EMAIL & MOSTAQL_PASSWORD are defined in your env or code
            email_input = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.NAME, "email")))
            email_input.send_keys(MOSTAQL_EMAIL)
            driver.find_element(By.NAME, "password").send_keys(MOSTAQL_PASSWORD)
            driver.find_element(By.XPATH, "//button[contains(text(),'Ø¯Ø®ÙˆÙ„')]").click()
            WebDriverWait(driver, 20).until(EC.not_(EC.url_contains("/login")))
            logging.info("Login successful!")
        else:
            logging.info("Already logged in via persistent Chrome profile.")
        return True
    except Exception:
        logging.exception("Selenium login process failed.")
        return False

def scrape_project_list(driver):
    logging.info("Scraping project list page...")
    try:
        driver.get(SEARCH_URL); time.sleep(5) # Ø²ÙŠØ§Ø¯Ø© ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹
        
        # âœ… ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: Ø¥Ø¶Ø§ÙØ© Ø³ÙƒØ±ÙŠÙ† Ø´ÙˆØª Ù„Ù„Ù€ Debug
        driver.save_screenshot("debug_page.png")
        logging.info("ğŸ“¸ Screenshot saved as debug_page.png (Check this if no jobs found)")

        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "tr.project-row")))
        soup = BeautifulSoup(driver.page_source, "lxml"); projects = []
        project_rows = soup.select('tr.project-row')
        for row in project_rows:
            link_tag = row.select_one("a[href*='/project/']")
            if not link_tag: continue
            
            bids_count = "0"
            bids_tag = row.select_one('li:-soup-contains("Ø¹Ø±Ø¶")')
            if bids_tag:
                bids_match = re.search(r'\d+', bids_tag.get_text())
                if bids_match: bids_count = bids_match.group(0)

            url = urljoin(BASE_URL, link_tag['href'])
            job_id_match = re.search(r'/project/(\d+)', url)
            if not job_id_match: continue
            
            projects.append({
                'id': job_id_match.group(1), 
                'title': link_tag.get_text(strip=True), 
                'url': url,
                'bids_count': bids_count
            })
        logging.info(f"âœ… Found {len(projects)} jobs on the page.")
        return projects
    except TimeoutException:
        logging.info("ğŸŸ¡ No jobs found on the page this time (Timeout)."); return []
    except Exception as e:
        logging.error(f"âŒ Error in scrape_project_list: {e}"); return []
    
def get_job_details(driver, job_url):
    try:
        driver.get(job_url)
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.carda__content")))
        soup = BeautifulSoup(driver.page_source, "lxml")
        
        description_tag = soup.select_one('div.carda__content')
        description = description_tag.get_text(separator="\n", strip=True) if description_tag else ""
        
        time_tag = soup.select_one('div.meta-value time')
        time_text = time_tag.get_text(strip=True) if time_tag else ""

        owner_details = {"name": "N/A", "joined": "N/A", "hire_rate": "N/A", "open_projects": "N/A"}
        owner_card = soup.select_one('div.profile_card')
        if owner_card:
            name_tag = owner_card.select_one('h5.profile__name bdi')
            if name_tag: owner_details['name'] = name_tag.get_text(strip=True)
            
            table_rows = owner_card.select('table.table-meta tr')
            for row in table_rows:
                cells = row.select('td')
                if len(cells) == 2:
                    key = cells[0].get_text(strip=True)
                    val = cells[1].get_text(strip=True)
                    if "ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„" in key: owner_details['joined'] = val
                    elif "Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªÙˆØ¸ÙŠÙ" in key: owner_details['hire_rate'] = val
                    elif "Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ù…ÙØªÙˆØ­Ø©" in key: owner_details['open_projects'] = val

        return {
            "description": description, 
            "time_text": time_text, 
            "full_html": driver.page_source,
            "owner_details": owner_details
        }
    except Exception as e:
        logging.warning(f"Failed to fetch details for {job_url}: {e}"); return None

def is_job_within_age_limit(time_text):
    if not time_text: return True
    try:
        text = time_text.lower()
        if any(s in text for s in ["Ø¯Ù‚ÙŠÙ‚Ø©", "Ø¯Ù‚Ø§Ø¦Ù‚", "minute", "minutes", "Ù„Ø­Ø¸Ø§Øª"]): return True
        if any(s in text for s in ["Ø³Ø§Ø¹Ø©", "Ø³Ø§Ø¹Ø§Øª", "hour", "hours"]):
            match = re.search(r'\d+', text)
            if match: return int(match.group()) <= MAX_AGE_HOURS
            return True
        if any(s in text for s in ["ÙŠÙˆÙ…", "Ø£ÙŠØ§Ù…", "day", "Ø´Ù‡Ø±", "month"]): return False
        return True
    except: return True

def parse_max_budget(html_content):
    try:
        soup = BeautifulSoup(html_content, "lxml")
        budget_tag = soup.select_one('div.meta-value[data-type="project-budget_range"]')
        if budget_tag:
            text = budget_tag.get_text().replace(",", "")
            numbers = [float(n) for n in re.findall(r'\d+\.\d+|\d+', text)]
            if numbers: return max(numbers)
    except: pass
    try:
        soup = BeautifulSoup(html_content, "lxml")
        full_text = soup.get_text(" ", strip=True).replace(",", "")
        patterns = [r"(\d+)\s*-\s*(\d+)\s*\$", r"\$\s*(\d+)\s*-\s*(\d+)", r"(\d+)\s*Ø¯ÙˆÙ„Ø§Ø±"]
        for pattern in patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                flat = [float(n) for sub in matches for n in (sub if isinstance(sub, tuple) else (sub,))]
                if flat: return max(flat)
    except: pass
    return None

# =================================================================================
# SECTION 6: TELEGRAM BOT
# =================================================================================
class TelegramBot(threading.Thread):
    def __init__(self, conn):
        super().__init__(name="TelegramBot", daemon=True)
        self.api_url = f"https://api.telegram.org/bot{TG_BOT_TOKEN}"
        self.conn = conn
        self.offset = None
        self.running = True

    def send_job_notification(self, chat_id, job):
        safe_title = html.escape(job.get('title', 'No Title'))
        safe_url = job.get('url', '')
        time_posted = html.escape(job.get('time_posted', ''))
        
        owner = job.get('owner', {})
        owner_info = (
            f"-----------------------------------\n"
            f"ğŸ‘¤ <b>Client:</b> {html.escape(owner.get('name', 'N/A'))} | "
            f"<b>Hire Rate:</b> {html.escape(owner.get('hire_rate', 'N/A'))}\n"
            f"-----------------------------------"
        )

        desc = html.escape(job.get('description', '')[:300] + "...")
        budget = f" | ğŸ’° ${job['max_budget']:.0f}" if job.get('max_budget') else ""
        
        # --- Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ ---
        ai_proposal = html.escape(job.get('ai_proposal', 'AI Proposal not generated.'))
        
        text = (
            f"âœ¨ <b>New Relevant Job</b> âœ¨\n\n"
            f"<b>Title:</b> <a href=\"{safe_url}\">{safe_title}</a>\n"
            f"<b>Category:</b> {job.get('category', 'General')}\n"
            f"<b>Posted:</b> {time_posted}{budget}\n"
            f"{owner_info}\n\n"
            f"<b>Description:</b>\n{desc}\n\n"
            f"ğŸ‘‡ <b>AI Suggested Proposal (Copy & Paste):</b>\n"
            f"<pre>{ai_proposal}</pre>"
        )
        
        reply_markup = {
            "inline_keyboard": [[
                {"text": "View Job â¡ï¸", "url": job['url']},
                {"text": "Applied âœ…", "callback_data": f"applied:{job['id']}"}
            ]]
        }
        
        try:
            payload = {
                "chat_id": chat_id,
                "text": text,
                "parse_mode": "HTML",
                "reply_markup": json.dumps(reply_markup),
                "disable_web_page_preview": True
            }
            requests.post(f"{self.api_url}/sendMessage", data=payload, timeout=15)
        except Exception as e:
            logging.error(f"Telegram Error: {e}")

    def run(self):
        logging.info("Telegram Bot poller started.")
        while self.running:
            try:
                params = {"timeout": 30, "offset": self.offset}
                response = requests.get(f"{self.api_url}/getUpdates", params=params, timeout=35)
                updates = response.json().get("result", [])
                for update in updates:
                    self.offset = update["update_id"] + 1
                    if "message" in update and update["message"].get("text") == "/start":
                        self.handle_start(update["message"])
            except: time.sleep(5)

    def handle_start(self, message):
        chat_id = str(message["chat"]["id"])
        with DB_CONNECTION_LOCK:
            self.conn.execute("INSERT OR IGNORE INTO subscribers (chat_id) VALUES (?)", (chat_id,))
            self.conn.commit()
        requests.post(f"{self.api_url}/sendMessage", data={"chat_id": chat_id, "text": "âœ… Subscribed!"})

    def stop(self): self.running = False

# =================================================================================
# SECTION 7: MAIN APPLICATION
# =================================================================================
def main():
    db_conn = get_db_connection(); init_db(db_conn)
    tg_bot = TelegramBot(db_conn); tg_bot.start()
    driver = None
    
    try:
        # âœ… ØªÙ… Ø§Ù„ØªØ¹Ø¯ÙŠÙ„: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­ Ø§Ù„Ø´Ø§Ù…Ù„Ø© (Headless + Stealth + Linux Compatibility)
        opts = Options()
        opts.add_argument(f"--user-data-dir={os.path.abspath(CHROME_PROFILE_PATH)}")
        opts.add_argument("--no-sandbox") # Ø¶Ø±ÙˆØ±ÙŠ Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù„ÙŠÙ†ÙƒØ³
        opts.add_argument("--disable-dev-shm-usage") # Ø¶Ø±ÙˆØ±ÙŠ Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù„ÙŠÙ†ÙƒØ³
        opts.add_argument("--window-size=1920,1080")
        
        if HEADLESS_BROWSER:
            opts.add_argument("--headless=new")
        
        # --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ®ÙÙŠ (Stealth Settings) ---
        opts.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
        opts.add_argument("--disable-blink-features=AutomationControlled")
        opts.add_experimental_option("excludeSwitches", ["enable-automation"])
        opts.add_experimental_option('useAutomationExtension', False)

        serv = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=serv, options=opts)
        
        # --- Ø­Ù‚Ù† ÙƒÙˆØ¯ Ø¬Ø§ÙØ§Ø³ÙƒØ±ÙŠØ¨Øª Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© ---
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        if not perform_login(driver): return
        
        while True:
            logging.info("--- Starting Check Cycle ---")
            
            # Get Subscribers
            with DB_CONNECTION_LOCK:
                db_subs = [str(r[0]) for r in db_conn.execute("SELECT chat_id FROM subscribers").fetchall()]
            all_subs = set(db_subs + PERMANENT_SUBSCRIBERS)

            # Scrape
            jobs = scrape_project_list(driver)
            if not jobs: time.sleep(POLL_INTERVAL_SECONDS); continue

            # Filter New Jobs
            new_ids = get_unseen_job_ids(db_conn, [j['id'] for j in jobs])
            new_jobs = [j for j in jobs if j['id'] in new_ids]
            
            logging.info(f"Found {len(new_jobs)} new jobs.")

            for job in new_jobs:
                details = get_job_details(driver, job['url'])
                if not details: continue
                
                # Filter 1: Age
                if not is_job_within_age_limit(details['time_text']): continue
                
                # Budget Parsing
                max_budget = parse_max_budget(details['full_html'])
                
                # Filter 2: Budget
                if max_budget is not None and max_budget < MIN_BUGET_USD: continue
                
                # --- Filter 3: Keywords Scoring (The Old Efficient Logic) ---
                full_text = job['title'] + " " + details['description']
                score, matches, category = calculate_relevance_score(full_text)
                
                if score < SEND_SCORE_THRESHOLD:
                    logging.info(f"Skipping job {job['title']} (Score: {score} - Not relevant)")
                    continue

                logging.info(f"Generating AI Proposal for: {job['title']}...")
                
                # --- Step 4: AI Proposal ---
                ai_proposal = generate_ai_proposal(details['description'], details['owner_details'].get('name', ''))
                
                job_data = {
                    **job,
                    "max_budget": max_budget or 0,
                    "description": details['description'],
                    "time_posted": details['time_text'],
                    "owner": details['owner_details'],
                    "ai_proposal": ai_proposal,
                    "score": score,
                    "category": category
                }
                
                # Send to Telegram
                for chat_id in all_subs:
                    tg_bot.send_job_notification(chat_id, job_data)
                
                # Save to DB
                with DB_CONNECTION_LOCK:
                    db_conn.execute(
                        "INSERT INTO jobs (id, title, url, first_seen, was_sent, score, max_budget) VALUES (?, ?, ?, ?, ?, ?, ?)",
                        (job['id'], job['title'], job['url'], datetime.utcnow(), 1, score, max_budget)
                    )
                
                time.sleep(2)

            logging.info(f"Cycle done. Waiting {POLL_INTERVAL_SECONDS}s...")
            time.sleep(POLL_INTERVAL_SECONDS)
            
    except KeyboardInterrupt:
        logging.info("Stopping...")
    finally:
        if driver: driver.quit()
        tg_bot.stop(); db_conn.close()

if __name__ == "__main__":
    main()
